{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de7053d",
   "metadata": {},
   "source": [
    "## Baseline Modeling for predictiong Learning Loss - Math ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483cd9d",
   "metadata": {},
   "source": [
    "The baseline models are:\n",
    "- Ridge Regression\n",
    "- SVM (Linear, Kernel)\n",
    "- KNN\n",
    "- Random Forest\n",
    "- Grandient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4b382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4b428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b966c",
   "metadata": {},
   "source": [
    "### Prepare data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debca0cd",
   "metadata": {},
   "source": [
    "Loading the data cleaned from [EDA.ipynb](..../processing/EDA.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ba8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656, 157)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../processing/DATA_Texas_District_v2_22.csv', sep=',', header=0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26591320",
   "metadata": {},
   "source": [
    "Loading the Feature Selection result report from [Feature_Selection_Math.ipynb](..../processing/Feature_Selection_Math.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7837d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.read_csv('../processing/Feature_Selection_Math_Report22.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83c3b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n Selected</th>\n",
       "      <th>type</th>\n",
       "      <th>Counts</th>\n",
       "      <th>Method</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1</th>\n",
       "      <th>features</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>139</td>\n",
       "      <td>by counts</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.374751</td>\n",
       "      <td>0.645275</td>\n",
       "      <td>['Staff:Students Diff 2019-2021', 'County Popu...</td>\n",
       "      <td>[[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>117</td>\n",
       "      <td>by methods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permutation Importance - Ridge</td>\n",
       "      <td>0.683206</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>0.369481</td>\n",
       "      <td>0.641345</td>\n",
       "      <td>['Staff:Students Diff 2019-2021', 'County Popu...</td>\n",
       "      <td>[[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>133</td>\n",
       "      <td>by counts</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.685115</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.350335</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>['Staff:Students Diff 2019-2021', 'County Popu...</td>\n",
       "      <td>[[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Grandient Boosting</td>\n",
       "      <td>34</td>\n",
       "      <td>by methods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permutation Importance - Random Forest</td>\n",
       "      <td>0.973282</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.349729</td>\n",
       "      <td>0.628380</td>\n",
       "      <td>['Avg Family Size 10', '% Total Operational Pu...</td>\n",
       "      <td>[[  3]\\n [ 10]\\n [ 11]\\n [ 18]\\n [ 20]\\n [ 22]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>139</td>\n",
       "      <td>by counts</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.679389</td>\n",
       "      <td>0.628788</td>\n",
       "      <td>0.338078</td>\n",
       "      <td>0.621687</td>\n",
       "      <td>['Staff:Students Diff 2019-2021', 'County Popu...</td>\n",
       "      <td>[[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  n Selected        type  Counts  \\\n",
       "48   Gradient Boosting         139   by counts     1.0   \n",
       "25               Ridge         117  by methods     NaN   \n",
       "17               Ridge         133   by counts     2.0   \n",
       "63  Grandient Boosting          34  by methods     NaN   \n",
       "16               Ridge         139   by counts     1.0   \n",
       "\n",
       "                                    Method  Train Accuracy  Test Accuracy  \\\n",
       "48                                     NaN        1.000000       0.659091   \n",
       "25          Permutation Importance - Ridge        0.683206       0.651515   \n",
       "17                                     NaN        0.685115       0.636364   \n",
       "63  Permutation Importance - Random Forest        0.973282       0.636364   \n",
       "16                                     NaN        0.679389       0.628788   \n",
       "\n",
       "         MCC        F1                                           features  \\\n",
       "48  0.374751  0.645275  ['Staff:Students Diff 2019-2021', 'County Popu...   \n",
       "25  0.369481  0.641345  ['Staff:Students Diff 2019-2021', 'County Popu...   \n",
       "17  0.350335  0.628200  ['Staff:Students Diff 2019-2021', 'County Popu...   \n",
       "63  0.349729  0.628380  ['Avg Family Size 10', '% Total Operational Pu...   \n",
       "16  0.338078  0.621687  ['Staff:Students Diff 2019-2021', 'County Popu...   \n",
       "\n",
       "                                                index  \n",
       "48  [[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...  \n",
       "25  [[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...  \n",
       "17  [[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...  \n",
       "63  [[  3]\\n [ 10]\\n [ 11]\\n [ 18]\\n [ 20]\\n [ 22]...  \n",
       "16  [[  0]\\n [  1]\\n [  2]\\n [  3]\\n [  4]\\n [  5]...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.sort_values(by=['MCC', 'F1'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94a593",
   "metadata": {},
   "source": [
    "**Get dummies for categorical feature `Locale`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e486664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656, 168)\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['Locale'], prefix='Locale')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66bdd3",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76202b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656, 145)\n"
     ]
    }
   ],
   "source": [
    "labels = ['Label_Math (21-22)', 'Label_Reading (21-22)', 'Label_All (21-22)']\n",
    "\n",
    "cols_drop = ['% Students Tested Reading - Grade 3 Diff 2021-2022',\n",
    "            '% Students Tested Reading - Grade 4 Diff 2021-2022',\n",
    "            '% Students Tested Reading - Grade 5 Diff 2021-2022',\n",
    "            '% Students Tested Reading - Grade 6 Diff 2021-2022',\n",
    "            '% Students Tested Reading - Grade 7 Diff 2021-2022',\n",
    "            '% Students Tested Reading - Grade 8 Diff 2021-2022',\n",
    "            '% Students Tested Reading - Grade 3 Diff 2019-2021',\n",
    "            '% Students Tested Reading - Grade 4 Diff 2019-2021',\n",
    "            '% Students Tested Reading - Grade 5 Diff 2019-2021',\n",
    "            '% Students Tested Reading - Grade 6 Diff 2019-2021',\n",
    "            '% Students Tested Reading - Grade 7 Diff 2019-2021',\n",
    "            '% Students Tested Reading - Grade 8 Diff 2019-2021',\n",
    "            '% Students Tested Reading - Grade 3 Diff 2019-2022',\n",
    "            '% Students Tested Reading - Grade 4 Diff 2019-2022',\n",
    "            '% Students Tested Reading - Grade 5 Diff 2019-2022',\n",
    "            '% Students Tested Reading - Grade 6 Diff 2019-2022',\n",
    "            '% Students Tested Reading - Grade 7 Diff 2019-2022',\n",
    "            '% Students Tested Reading - Grade 8 Diff 2019-2022',\n",
    "            'District #', 'County #']\n",
    "\n",
    "y = df['Label_Math (21-22)'].values\n",
    "X = df.drop(columns=labels + cols_drop).values\n",
    "columns = df.drop(columns=labels + cols_drop).columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y,\n",
    "                                                    random_state=123, shuffle=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6c3ec58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>302</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Train  Test\n",
       "0     -1    120    30\n",
       "1      0    302    76\n",
       "2      1    102    26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(y_train, return_counts=True)\n",
    "values_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "pd.DataFrame({'Label': values.tolist(), 'Train': counts.tolist(), 'Test': counts_test.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde5d24",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851ae302",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75410e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=123\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfed6b7",
   "metadata": {},
   "source": [
    "### Experiment with the Feature Sets selected from [Feature_Selection_Math.ipynb](..../processing/Feature_Selection_Math.ipynb) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84301263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set 1 (Original Set)\n",
      "\tn features: 145\n",
      "Feature Set 2\n",
      "\tn features: 23\n",
      "Feature Set 3\n",
      "\tn features: 31\n",
      "Feature Set 4\n",
      "\tn features: 34\n",
      "Feature Set 5\n",
      "\tn features: 50\n",
      "Feature Set 6\n",
      "\tn features: 69\n",
      "Feature Set 7\n",
      "\tn features: 69\n",
      "Feature Set 8\n",
      "\tn features: 70\n",
      "Feature Set 9\n",
      "\tn features: 117\n",
      "Feature Set 10\n",
      "\tn features: 119\n"
     ]
    }
   ],
   "source": [
    "print('Feature Set 1 (Original Set)')\n",
    "print('\\tn features:', len(columns))\n",
    "\n",
    "feature_set = report[(report['model'] == 'Ridge') & (report['type'] == 'by methods')].sort_values(by=['n Selected'], ignore_index=True)\n",
    "n = feature_set.shape[0]\n",
    "\n",
    "for i in range(n):\n",
    "    print('Feature Set', str(i+2))\n",
    "    print('\\tn features: {:0.0f}'.format(feature_set.loc[i, 'n Selected']))\n",
    "    if feature_set.loc[i, 'type'] == 'by counts': \n",
    "        print('\\tSelected by {:0.0f} methods'.format(feature_set.loc[i, 'Counts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be0d19",
   "metadata": {},
   "source": [
    "### Modeling ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bed6e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(estimator, param_grid, name):\n",
    "    result = pd.DataFrame()\n",
    "    avg = 'weighted'  \n",
    "     \n",
    "    grid = GridSearchCV(estimator, param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        if i == n: \n",
    "            features = columns\n",
    "            method = 'No Reduction'\n",
    "        else: \n",
    "            features = list(feature_set.loc[i, 'features'].strip(\"[]''\").split(\"', '\"))\n",
    "            method = feature_set.loc[i, 'Method']\n",
    "        \n",
    "        index = np.argwhere(np.isin(columns, features))\n",
    "        X_train2 = X_train[:, index.flatten()]\n",
    "        X_test2 = X_test[:, index.flatten()]\n",
    "        \n",
    "        t_start = time.process_time()\n",
    "        grid.fit(X_train2, y_train)\n",
    "        train_time = time.process_time() - t_start\n",
    "        y_pred = grid.best_estimator_.predict(X_test2)\n",
    "        temp = {'Model': name,\n",
    "                'Method': method,\n",
    "                'n Selected': int(index.shape[0]),\n",
    "                'best_params': grid.best_params_,\n",
    "                'Best Accuracy': grid.best_score_, \n",
    "                'Train Accuracy': grid.best_estimator_.score(X_train2, y_train),\n",
    "                'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred, average=avg),\n",
    "                'Recall': recall_score(y_test, y_pred, average=avg),\n",
    "                'F1': f1_score(y_test, y_pred, average=avg),\n",
    "                'MCC': matthews_corrcoef(y_test, y_pred), \n",
    "                'ROC': roc_auc_score(y_test, grid.best_estimator_.predict_proba(X_test2), multi_class='ovr', average=avg),\n",
    "                'conf_mat': confusion_matrix(y_test, y_pred), \n",
    "                'prediction': y_pred,\n",
    "                'predict_proba': grid.best_estimator_.predict_proba(X_test2),\n",
    "                'features': features,\n",
    "                'estimator': grid.best_estimator_,\n",
    "                'Train Time': train_time,\n",
    "                'classification_report': classification_report(y_test, y_pred)}\n",
    "        result = pd.concat([result, pd.DataFrame([temp])], ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628d5ea",
   "metadata": {},
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d93ff449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "params={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "estimator=LogisticRegression(penalty='l2', n_jobs=-1, random_state=random_state, max_iter=10000, multi_class='ovr')\n",
    "result=model_report(estimator, params, 'Ridge')\n",
    "results=pd.concat([results, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a3cfc",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee95df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf']}\n",
    "        \n",
    "estimator=SVC(random_state=random_state, probability=True, decision_function_shape='ovr')\n",
    "result=model_report(estimator, params, 'SVM')\n",
    "results=pd.concat([results, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73f5f3",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a115db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "params = {'n_neighbors': [1,3,5,7,9,11],\n",
    "          'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "          'leaf_size': [10, 30, 50]}\n",
    "          \n",
    "estimator=KNeighborsClassifier(n_jobs=-1)\n",
    "result=model_report(estimator, params, 'KNN')\n",
    "results=pd.concat([results, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795864af",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97793e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 13min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = {'max_depth': [1, 6, None],\n",
    "          'n_estimators': [50, 100, 200],\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'min_samples_leaf': [1, 5, 10],    \n",
    "          'max_samples': [0.1, 0.5, None],\n",
    "          'max_leaf_nodes': [10, 31, None],\n",
    "          'ccp_alpha': [0, 0.001, 0.1]}\n",
    "          \n",
    "estimator=RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "result=model_report(estimator, params, 'RF')\n",
    "results=pd.concat([results, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb3bc8",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd354d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "params = {'learning_rate': [0.1, 0.2, 0.3],\n",
    "           'n_estimators': [50, 100, 200],\n",
    "           'min_samples_leaf': [1, 5, 10],\n",
    "           'min_weight_fraction_leaf': [0.0, 0.1, 0.5],\n",
    "           'max_depth': [1, 3, 6], \n",
    "           'max_leaf_nodes': [10, 31, None],\n",
    "           'ccp_alpha': [0, 0.001, 0.1]}\n",
    "\n",
    "estimator=GradientBoostingClassifier(random_state=random_state, n_iter_no_change=20)\n",
    "result=model_report(estimator, params, 'GB')\n",
    "results=pd.concat([results, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8dffe3",
   "metadata": {},
   "source": [
    "## Result  ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e67e2",
   "metadata": {},
   "source": [
    "Top 10 results are sorted by `MCC` and `F1` score for test set. Among the top 10, grandient boosting and random forest are the most frequently appeared for the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83492a74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=['MCC', 'F1'], ascending=False).head(10)[['Model', 'Method', 'n Selected', 'Test Accuracy', 'Precision', 'Recall', 'MCC', 'F1', 'best_params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283317b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=['MCC', 'F1'], ascending=False).head(10)['Model'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c918cf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=['MCC', 'F1'], ascending=False).head(10)['n Selected'].value_counts().sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599417d",
   "metadata": {},
   "source": [
    "**Accuracy, MCC and F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1afec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = pd.DataFrame()\n",
    "for col in ['Train Accuracy', 'Test Accuracy', 'MCC', 'F1']:\n",
    "    temp = results[['Model', 'n Selected'] + [col]].copy()\n",
    "    temp['Score'] = col\n",
    "    temp.rename(columns={col: 'Score value'}, inplace=True)\n",
    "    results2 = pd.concat([results2, temp], ignore_index=True)\n",
    "results2=results2.astype({'n Selected': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.set(font_scale=1.8)\n",
    "\n",
    "g = sns.catplot(x=\"n Selected\", y=\"Score value\", hue=\"Score\", col=\"Model\",\n",
    "                capsize=.2, palette=\"magma\", height=6, aspect=.75,\n",
    "                kind=\"point\", data=results2)\n",
    "g.despine(left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = results['Model'].unique().tolist()\n",
    "temp=results.sort_values(by=['Model', 'MCC', 'F1'], ascending=False)\n",
    "\n",
    "best_per_model_idx = []\n",
    "for i in range(len(temp.index)):\n",
    "    if i % (n+1) == 0: best_per_model_idx.append(temp.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6d639",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def ticks(x, position):\n",
    "    if position == 1: return 'Loss'\n",
    "    if position == 2: return 'Exp'\n",
    "    if position == 3: return 'Gain'\n",
    "    \n",
    "fig, axes=plt.subplots(1,len(best_per_model_idx), figsize=(20,10))\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "for idx, i in zip(sorted(best_per_model_idx), range(len(best_per_model_idx))):\n",
    "    plot_confusion_matrix(axis=axes[i], conf_mat=results.loc[idx, 'conf_mat'],\n",
    "                          show_absolute=True, show_normed=True)\n",
    "    title='{} (n={:0.0f})'.format(results.loc[idx, 'Model'], results.loc[idx, 'n Selected'])\n",
    "    axes[i].set_title(title, fontsize=20)\n",
    "    if (i > 0): axes[i].set(xlabel='', ylabel='')\n",
    "    else : axes[i].set(xlabel='Predicted Label', ylabel='True Label')\n",
    "    axes[i].xaxis.set_major_formatter(mticker.FuncFormatter(ticks))\n",
    "    axes[i].yaxis.set_major_formatter(mticker.FuncFormatter(ticks))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a49870",
   "metadata": {},
   "source": [
    "**ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "classes=[-1, 0, 1]\n",
    "y_test_bin=label_binarize(y_test, classes=classes)\n",
    "\n",
    "fig, axes=plt.subplots(1,len(best_per_model_idx), figsize=(20,5))\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "for idx, i in zip(sorted(best_per_model_idx), range(len(best_per_model_idx))):\n",
    "    fpr, tpr = {}, {}\n",
    "\n",
    "    for c in classes:\n",
    "        fpr[c], tpr[c], _ = roc_curve(y_test_bin[:, c+1], results.loc[idx, 'predict_proba'][:, c+1])\n",
    "        axes[i].plot(fpr[c], tpr[c], lw=2, label='{} (AUC={:0.2f})'.format(ticks(c,c+2), auc(fpr[c], tpr[c])))\n",
    "    \n",
    "    title='{} (n={:0.0f})'.format(results.loc[idx, 'Model'], results.loc[idx, 'n Selected'])\n",
    "    axes[i].set_title(title, fontsize=20)\n",
    "    axes[i].set_xlabel(\"False Postive Rate\")\n",
    "    axes[0].set_ylabel(\"True Positive Rate\")\n",
    "    axes[i].legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbd746",
   "metadata": {},
   "source": [
    "**Precision-Recall Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b421842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "classes=[-1, 0, 1]\n",
    "y_test_bin=label_binarize(y_test, classes=classes)\n",
    "\n",
    "fig, axes=plt.subplots(1,len(best_per_model_idx), figsize=(20,5))\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "for idx, i in zip(sorted(best_per_model_idx), range(len(best_per_model_idx))):\n",
    "    prec, rec = {}, {}\n",
    "\n",
    "    for c in classes:\n",
    "        prec[c], rec[c], _ = precision_recall_curve(y_test_bin[:, c+1], results.loc[idx, 'predict_proba'][:, c+1])\n",
    "        axes[i].plot(rec[c], prec[c], lw=2, label='{}'.format(ticks(c,c+2)))\n",
    "    \n",
    "    title='{} (n={:0.0f})'.format(results.loc[idx, 'Model'], results.loc[idx, 'n Selected'])\n",
    "    axes[i].set_title(title, fontsize=20)\n",
    "    axes[i].set_xlabel(\"Recall\")\n",
    "    axes[0].set_ylabel(\"Precision\")\n",
    "    axes[i].legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='MCC', ascending=False).to_csv('Modeling_BL_Math22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf536e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b530cbb57870e8c17e0b147b89ef717ce29e19d06452b992ddf605f8bc0a986d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
